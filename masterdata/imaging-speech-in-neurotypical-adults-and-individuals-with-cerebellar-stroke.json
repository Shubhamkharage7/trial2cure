{
    "protocolSection": {
        "identificationModule": {
            "nctId": "NCT06458153",
            "orgStudyIdInfo": {
                "id": "STUDY23070083"
            },
            "secondaryIdInfos": [
                {
                    "id": "1R01DC020963-01A1",
                    "type": "NIH",
                    "link": "https://reporter.nih.gov/quickSearch/1R01DC020963-01A1"
                }
            ],
            "organization": {
                "fullName": "University of Pittsburgh",
                "class": "OTHER"
            },
            "briefTitle": "Imaging Speech in Neurotypical Adults and Individuals With Cerebellar Stroke",
            "officialTitle": "High-resolution Functional Imaging of Speech-induced Sensory Modulation",
            "therapeuticArea": [
                "Other",
                "Neurology"
            ],
            "study": "imaging-speech-in-neurotypical-adults-and-individuals-with-cerebellar-stroke"
        },
        "statusModule": {
            "statusVerifiedDate": "2024-06",
            "overallStatus": "NOT_YET_RECRUITING",
            "expandedAccessInfo": {
                "hasExpandedAccess": false
            },
            "startDateStruct": {
                "date": "2024-07",
                "type": "ESTIMATED"
            },
            "primaryCompletionDateStruct": {
                "date": "2028-07",
                "type": "ESTIMATED"
            },
            "completionDateStruct": {
                "date": "2028-07",
                "type": "ESTIMATED"
            },
            "studyFirstSubmitDate": "2024-06-03",
            "studyFirstSubmitQcDate": "2024-06-07",
            "studyFirstPostDateStruct": {
                "date": "2024-06-13",
                "type": "ACTUAL"
            },
            "lastUpdateSubmitDate": "2024-06-07",
            "lastUpdatePostDateStruct": {
                "date": "2024-06-13",
                "type": "ACTUAL"
            }
        },
        "sponsorCollaboratorsModule": {
            "responsibleParty": {
                "type": "PRINCIPAL_INVESTIGATOR",
                "investigatorFullName": "Jason Bohland",
                "investigatorTitle": "Assistant Professor",
                "investigatorAffiliation": "University of Pittsburgh"
            },
            "leadSponsor": {
                "name": "University of Pittsburgh",
                "class": "OTHER"
            },
            "collaborators": [
                {
                    "name": "Northwestern University",
                    "class": "OTHER"
                },
                {
                    "name": "National Institute on Deafness and Other Communication Disorders (NIDCD)",
                    "class": "NIH"
                }
            ]
        },
        "oversightModule": {
            "oversightHasDmc": false,
            "isFdaRegulatedDrug": false,
            "isFdaRegulatedDevice": false
        },
        "descriptionModule": {
            "briefSummary": "The goal of this research study is to learn how the brain areas that plan and control movement interact with the areas responsible for hearing and perceiving speech in healthy adults and people who have had cerebellar strokes. The main questions it aims to answer are:\n\n1. What regions of the brain's sensory systems show changes in their activity related to speech?\n2. To what extent do these regions help listeners detect and correct speech errors?\n3. What is the role of the cerebellum (a part of the brain in the back of the head) in these activities?\n\nParticipants will be asked to complete several experimental sessions involving behavioral speech and related tests and non-invasive brain imaging using electroencephalography (EEG) and functional magnetic resonance imaging (fMRI).",
            "detailedDescription": "This study aims to provide an integrated view of brain systems underlying predictive coding in speech with unprecedented detail using ultra-high field (7 Tesla) functional magnetic resonance imaging. The overall approach is a condition-intensive within-subjects design, with extensive sampling of individual participants, including a group who have had strokes impacting the cerebellum, across multiple sessions.\n\nParticipants will be asked to complete up to 6 sessions. Passing a hearing assessment using standard audiological procedures, conducted at the start of the first session, is a requirement for participation. The experimental sessions involve behavior and non-invasive brain imaging.\n\nInvestigators will ask participants to perform several short tasks to measure different aspects of their speech production and speech perception (e.g., reading passages or words aloud, making judgements about sounds).\n\nIn one session, Investigators will measure electroencephalography (EEG) while participants complete tasks involving producing and hearing speech sounds. Participants will be fitted with an elastic cap and up to 32 non-invasive recording electrodes.\n\nIn other sessions, investigators will measure structural and functional magnetic resonance imaging (fMRI). Structural images demonstrate the unique brain anatomy of the participant. Functional images will be obtained while the participant completes specific tasks involving listening, speaking, or completing other motor actions (e.g., pressing a button). All participants will be screened for MRI risk factors prior to each session."
        },
        "conditionsModule": {
            "conditions": [
                "Stroke",
                "Cerebellum"
            ],
            "keywords": [
                "fmri",
                "speech production",
                "cerebellum",
                "auditory-motor integration",
                "speech motor control",
                "neuroimaging"
            ]
        },
        "designModule": {
            "studyType": "INTERVENTIONAL",
            "phases": [
                "NA"
            ],
            "designInfo": {
                "allocation": "NA",
                "interventionModel": "SINGLE_GROUP",
                "interventionModelDescription": "The single group model involves assessing the neural basis of multiple behavioral tasks in three cohorts. Cohort 1 is neurotypical adults. Cohort 2 is people with post-stroke cerebellar lesions. Cohort 3 is a group of control participants that will be matched to Cohort 2. The behavioral interventions are overlapping between cohorts, though Cohort 1 will complete additional sessions.",
                "primaryPurpose": "BASIC_SCIENCE",
                "maskingInfo": {
                    "masking": "NONE"
                }
            },
            "enrollmentInfo": {
                "count": 100,
                "type": "ESTIMATED"
            }
        },
        "armsInterventionsModule": {
            "armGroups": [
                {
                    "label": "Speech behavior and functional imaging",
                    "type": "EXPERIMENTAL",
                    "description": "Assessing the neural correlates of speaking-induced sensory modulation in all three cohorts using behavior and neuroimaging tasks in up to 6 sessions.",
                    "interventionNames": [
                        "Behavioral: Neural responses to speech functional localizer",
                        "Behavioral: Neural responses to silent articulation",
                        "Behavioral: Neural responses to self vs. externally generated speech",
                        "Behavioral: Event-related potentials for speech",
                        "Behavioral: Neural responses to induced speech errors",
                        "Behavioral: Neural responses to sensory-motor adaptation",
                        "Behavioral: Speech production behaviors",
                        "Behavioral: Auditory acuity testing",
                        "Behavioral: Neural responses to learning a non-speech auditory motor behavior"
                    ]
                }
            ],
            "interventions": [
                {
                    "type": "BEHAVIORAL",
                    "name": "Neural responses to speech functional localizer",
                    "description": "Measuring speech-related brain activity using fMRI during a speech listening task.",
                    "armGroupLabels": [
                        "Speech behavior and functional imaging"
                    ]
                },
                {
                    "type": "BEHAVIORAL",
                    "name": "Neural responses to silent articulation",
                    "description": "Measuring speech-related brain activity using fMRI during a silent articulation task.",
                    "armGroupLabels": [
                        "Speech behavior and functional imaging"
                    ]
                },
                {
                    "type": "BEHAVIORAL",
                    "name": "Neural responses to self vs. externally generated speech",
                    "description": "Measuring speech-related brain activity using fMRI during self-generated vs. externally-generated speech.",
                    "armGroupLabels": [
                        "Speech behavior and functional imaging"
                    ]
                },
                {
                    "type": "BEHAVIORAL",
                    "name": "Event-related potentials for speech",
                    "description": "Measuring electroencephalography (EEG) based evoked potentials for self vs. externally generated speech",
                    "armGroupLabels": [
                        "Speech behavior and functional imaging"
                    ]
                },
                {
                    "type": "BEHAVIORAL",
                    "name": "Neural responses to induced speech errors",
                    "description": "Measuring speech-related brain activity using fMRI during conditions that induce auditory speech errors.",
                    "armGroupLabels": [
                        "Speech behavior and functional imaging"
                    ]
                },
                {
                    "type": "BEHAVIORAL",
                    "name": "Neural responses to sensory-motor adaptation",
                    "description": "Measuring brain activity using fMRI during a learning task with sustained altered auditory feedback.",
                    "armGroupLabels": [
                        "Speech behavior and functional imaging"
                    ]
                },
                {
                    "type": "BEHAVIORAL",
                    "name": "Speech production behaviors",
                    "description": "Behavioral measurements of speech during reading passages and words",
                    "armGroupLabels": [
                        "Speech behavior and functional imaging"
                    ]
                },
                {
                    "type": "BEHAVIORAL",
                    "name": "Auditory acuity testing",
                    "description": "Measurements of auditory acuity during listening tasks.",
                    "armGroupLabels": [
                        "Speech behavior and functional imaging"
                    ]
                },
                {
                    "type": "BEHAVIORAL",
                    "name": "Neural responses to learning a non-speech auditory motor behavior",
                    "description": "Mapping of brain areas using fMRI during learning of non-speech sound-evoking movements.",
                    "armGroupLabels": [
                        "Speech behavior and functional imaging"
                    ]
                }
            ]
        },
        "outcomesModule": {
            "primaryOutcomes": [
                {
                    "measure": "Blood oxygenation level dependent (BOLD) responses to self vs. externally generated speech",
                    "description": "The dependent variables (across voxels) are blood oxygenated level dependent fMRI measurements made during task performance. We will contrast measured activations in regions of interest for the LISTEN-SELF vs. PRODUCE and LISTEN-OTHER vs. PRODUCE conditions. Encoding models will predict activity in regions-of-interest (ROIs) based on a set of speech features.",
                    "timeFrame": "One session lasting 2-3 hours, within 12 months of enrollment"
                },
                {
                    "measure": "BOLD responses related to pre-speech auditory modulation",
                    "description": "The dependent variables (across voxels) are blood oxygenated level dependent fMRI measurements made during task performance. We will contrast measured activations in regions of interest for responses to auditory stimuli across conditions (e.g., SPEAK, REHEARSE, PLAN, SILENT).",
                    "timeFrame": "One session lasting 2-3 hours, within 12 months of enrollment"
                },
                {
                    "measure": "EEG responses to self vs. externally generated speech",
                    "description": "The dependent variables are evoked responses, aligned to sound onset, measured with EEG during task performance. We will contrast evoked responses across conditions (e.g., TALK, LISTEN).",
                    "timeFrame": "One session lasting 2-3 hours, within 12 months of enrollment"
                },
                {
                    "measure": "BOLD responses to induced auditory errors",
                    "description": "The dependent variables (across voxels) are blood oxygenated level dependent fMRI measurements made during task performance. We will determine activations in regions of interest that correlate with applied perturbations during speech. We will also compare SPEAK vs. LISTEN activations in perturbed and unperturbed conditions.",
                    "timeFrame": "One session lasting 2-3 hours, within 12 months of enrollment"
                },
                {
                    "measure": "BOLD responses during adaptation to auditory perturbations",
                    "description": "The dependent variables (across voxels) are blood oxygenated level dependent fMRI measurements made during task performance. We will contrast measured activations in regions of interest for responses during the HOLD and BASELINE phases of the adaptation paradigm. We will determine areas where activation is associated with changes in formant frequencies in early and late windows in speech recordings.",
                    "timeFrame": "One session lasting 2-3 hours, within 12 months of enrollment"
                },
                {
                    "measure": "BOLD responses during learning of non-speech auditory motor targets",
                    "description": "The dependent variables (across voxels) are blood oxygenated level dependent fMRI measurements made during task performance. We will contrast measured activations in regions of interest for responses during PRESS trials across runs. We will contrast LISTEN vs. PRESS trials to measure motor induced sensory modulation.",
                    "timeFrame": "One session lasting 2-3 hours, within 12 months of enrollment"
                }
            ],
            "secondaryOutcomes": [
                {
                    "measure": "BOLD responses to speech listening task",
                    "description": "The dependent variables (across voxels) are blood oxygenated level dependent fMRI measurements made during task performance. We will contrast measured activations for the SPEECH vs. signal correlated noise (SCN) and SPEECH vs. SILENT conditions.",
                    "timeFrame": "One session lasting 2-3 hours, within 12 months of enrollment"
                },
                {
                    "measure": "BOLD responses to silent articulation task",
                    "description": "The dependent variables (across voxels) are blood oxygenated level dependent fMRI measurements made during task performance. We will contrast measured activations for silent articulation vs. a resting baseline condition.",
                    "timeFrame": "One session lasting 2-3 hours, within 12 months of enrollment"
                },
                {
                    "measure": "Speech formant frequencies",
                    "description": "We will measure participant-specific phonetic variables (formant frequencies) based on participant speech from reading passages and word production.",
                    "timeFrame": "First session lasting 2-3 hours, within 12 months of enrollment"
                },
                {
                    "measure": "Spontaneous Speech Synchronization Index",
                    "description": "We will measure the Spontaneous Speech Synchronization Index based on behavioral speech data.",
                    "timeFrame": "First session lasting 2-3 hours, within 12 months of enrollment"
                },
                {
                    "measure": "Auditory acuity",
                    "description": "We will measure auditory acuity (just noticeable difference) for changes in formant frequencies based on behavioral speech samples.",
                    "timeFrame": "First session lasting 2-3 hours, within 12 months of enrollment"
                }
            ]
        },
        "eligibilityModule": {
            "eligibilityCriteria": "Inclusion Criteria:\n\nCohort 1 (neurotypical adults):\n\n* Age 18-49\n* Right-handed\n* Native English speaker\n\nCohort 2 (people with cerebellar lesions):\n\n* Age 18 or older\n* Right-handed\n* Native English speaker\n* History of cerebellar stroke\n\nCohort 3 (controls matched to Cohort 2)\n\n* Age 18 or older\n* Right-handed\n* Native English speaker\n\nExclusion Criteria:\n\nCohort 1 (neurotypical adults):\n\n* Presence of MRI risk factors: metal and/or electromagnetic devices (e.g., pacemakers, neurostimulators) in the body, previous shrapnel injuries, use of an intrauterine device containing metal, claustrophobia, pregnant or possibly pregnant\n* History of neurological disease, injury, or impairment\n* Hearing loss, defined by pure tone thresholds \\>25 decibels (dB) hearing level (HL) at octave frequencies between 250-8000 Hz\n* Clinically diagnosed with or treated for a neuropsychiatric disorder\n* Clinically diagnosed with or treated for a speech, language, or hearing disorder\n* Head circumference greater than 60cm or weight greater than 300 pounds\n* History of claustrophobia\n* Currently pregnant\n\nCohort 2 (people with cerebellar lesions):\n\n* Presence of MRI risk factors: metal and/or electromagnetic devices (e.g., pacemakers, neurostimulators) in the body, previous shrapnel injuries, use of an intrauterine device containing metal, claustrophobia, pregnant or possibly pregnant\n* Hearing loss, defined by pure tone thresholds \\>50 dB HL at octave frequencies between 250-4000 Hz\n* Clinically diagnosed with or treated for a neuropsychiatric disorder\n* Head circumference greater than 60cm or weight greater than 300 pounds\n* History of claustrophobia\n* Currently pregnant\n\nCohort 3 (controls matched to Cohort 2):\n\n* Presence of MRI risk factors: metal and/or electromagnetic devices (e.g., pacemakers, neurostimulators) in the body, previous shrapnel injuries, use of an intrauterine device containing metal, claustrophobia, pregnant or possibly pregnant\n* History of neurological disease, injury, or impairment\n* Hearing loss, defined by pure tone thresholds \\>50 dB HL at octave frequencies between 250-4000 Hz\n* Clinically diagnosed with or treated for a neuropsychiatric disorder\n* Clinically diagnosed with or treated for a speech, language, or hearing disorder\n* Head circumference greater than 60cm or weight greater than 300 pounds\n* History of claustrophobia\n* Currently pregnant",
            "healthyVolunteers": true,
            "sex": "ALL",
            "minimumAge": "18 Years",
            "stdAges": [
                "ADULT",
                "OLDER_ADULT"
            ]
        },
        "contactsLocationsModule": {
            "centralContacts": [
                {
                    "name": "Jason W Bohland, Ph.D.",
                    "role": "CONTACT",
                    "phone": "412-383-3416",
                    "email": "j.bohland@pitt.edu"
                },
                {
                    "name": "Alexander Ocampo, B.A.",
                    "role": "CONTACT",
                    "email": "amo104@pitt.edu"
                }
            ],
            "overallOfficials": [
                {
                    "name": "Jason W Bohland, Ph.D.",
                    "affiliation": "University of Pittsburgh",
                    "role": "PRINCIPAL_INVESTIGATOR"
                }
            ],
            "locations": [
                {
                    "facility": "University of Pittsburgh",
                    "city": "Pittsburgh",
                    "state": "Pennsylvania",
                    "zip": "15260",
                    "country": "United States",
                    "contacts": [
                        {
                            "name": "Jason Bohland, Ph.D.",
                            "role": "CONTACT",
                            "phone": "412-383-3416",
                            "email": "j.bohland@pitt.edu"
                        },
                        {
                            "name": "Alexander Ocampo, BA",
                            "role": "CONTACT",
                            "email": "amo104@pitt.edu"
                        },
                        {
                            "name": "Jason W Bohland, Ph.D.",
                            "role": "PRINCIPAL_INVESTIGATOR"
                        }
                    ],
                    "geoPoint": {
                        "lat": 40.44062,
                        "lon": -79.99589
                    }
                }
            ]
        },
        "ipdSharingStatementModule": {
            "ipdSharing": "YES",
            "description": "The study will produce behavioral, audio, electroencephalography (EEG), and structural and functional magnetic resonance imaging (MRI) data. Data will be de-identified (MRI images will be defaced and all identifiers will be scrubbed) and made available for sharing at the individual level using community accepted formats for data and metadata.",
            "infoTypes": [
                "STUDY_PROTOCOL",
                "SAP",
                "ANALYTIC_CODE"
            ],
            "timeFrame": "Data will be made available as soon as possible or at the time of associated publication. The duration of preservation and sharing of the data will be a minimum of 7 years after the end of study funding.",
            "accessCriteria": "Individual-level raw MRI and EEG datasets (along with metadata necessary to replicate analyses) will be shared via OpenNeuro.org, an open access repository for sharing neuroimaging data in Brain Imaging Data Structure (BIDS) format. Where possible, behavioral and audio data will also be shared on OpenNeuro.org, made available in a different free repository (Open Science Framework), or made available upon request.",
            "url": "https://openneuro.org/"
        }
    },
    "derivedSection": {
        "miscInfoModule": {
            "versionHolder": "2024-07-30"
        },
        "conditionBrowseModule": {
            "meshes": [
                {
                    "id": "D000020521",
                    "term": "Stroke"
                }
            ],
            "ancestors": [
                {
                    "id": "D000002561",
                    "term": "Cerebrovascular Disorders"
                },
                {
                    "id": "D000001927",
                    "term": "Brain Diseases"
                },
                {
                    "id": "D000002493",
                    "term": "Central Nervous System Diseases"
                },
                {
                    "id": "D000009422",
                    "term": "Nervous System Diseases"
                },
                {
                    "id": "D000014652",
                    "term": "Vascular Diseases"
                },
                {
                    "id": "D000002318",
                    "term": "Cardiovascular Diseases"
                }
            ],
            "browseLeaves": [
                {
                    "id": "M22306",
                    "name": "Stroke",
                    "asFound": "Stroke",
                    "relevance": "HIGH"
                },
                {
                    "id": "M5810",
                    "name": "Cerebrovascular Disorders",
                    "relevance": "LOW"
                },
                {
                    "id": "M5204",
                    "name": "Brain Diseases",
                    "relevance": "LOW"
                },
                {
                    "id": "M5742",
                    "name": "Central Nervous System Diseases",
                    "relevance": "LOW"
                },
                {
                    "id": "M17400",
                    "name": "Vascular Diseases",
                    "relevance": "LOW"
                }
            ],
            "browseBranches": [
                {
                    "abbrev": "BC10",
                    "name": "Nervous System Diseases"
                },
                {
                    "abbrev": "BC14",
                    "name": "Heart and Blood Diseases"
                },
                {
                    "abbrev": "All",
                    "name": "All Conditions"
                }
            ]
        }
    },
    "hasResults": false
}