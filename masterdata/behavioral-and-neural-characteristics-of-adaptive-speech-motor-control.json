{
    "protocolSection": {
        "identificationModule": {
            "nctId": "NCT06164717",
            "orgStudyIdInfo": {
                "id": "STUDY00001367"
            },
            "secondaryIdInfos": [
                {
                    "id": "R01DC020707",
                    "type": "NIH",
                    "link": "https://reporter.nih.gov/quickSearch/R01DC020707"
                },
                {
                    "id": "R01DC014510",
                    "type": "NIH",
                    "link": "https://reporter.nih.gov/quickSearch/R01DC014510"
                }
            ],
            "organization": {
                "fullName": "University of Washington",
                "class": "OTHER"
            },
            "briefTitle": "Behavioral and Neural Characteristics of Adaptive Speech Motor Control",
            "officialTitle": "Behavioral and Neural Characteristics of Adaptive Speech Motor Control",
            "therapeuticArea": [
                "Other"
            ],
            "study": "behavioral-and-neural-characteristics-of-adaptive-speech-motor-control"
        },
        "statusModule": {
            "statusVerifiedDate": "2023-12",
            "overallStatus": "RECRUITING",
            "expandedAccessInfo": {
                "hasExpandedAccess": false
            },
            "startDateStruct": {
                "date": "2023-01-01",
                "type": "ACTUAL"
            },
            "primaryCompletionDateStruct": {
                "date": "2027-12-31",
                "type": "ESTIMATED"
            },
            "completionDateStruct": {
                "date": "2027-12-31",
                "type": "ESTIMATED"
            },
            "studyFirstSubmitDate": "2023-11-22",
            "studyFirstSubmitQcDate": "2023-12-01",
            "studyFirstPostDateStruct": {
                "date": "2023-12-11",
                "type": "ACTUAL"
            },
            "lastUpdateSubmitDate": "2023-12-01",
            "lastUpdatePostDateStruct": {
                "date": "2023-12-11",
                "type": "ACTUAL"
            }
        },
        "sponsorCollaboratorsModule": {
            "responsibleParty": {
                "type": "PRINCIPAL_INVESTIGATOR",
                "investigatorFullName": "Ludo Max",
                "investigatorTitle": "Professor, Department of Speech and Hearing Sciences",
                "investigatorAffiliation": "University of Washington"
            },
            "leadSponsor": {
                "name": "University of Washington",
                "class": "OTHER"
            },
            "collaborators": [
                {
                    "name": "National Institutes of Health (NIH)",
                    "class": "NIH"
                },
                {
                    "name": "National Institute on Deafness and Other Communication Disorders (NIDCD)",
                    "class": "NIH"
                }
            ]
        },
        "oversightModule": {
            "oversightHasDmc": false,
            "isFdaRegulatedDrug": false,
            "isFdaRegulatedDevice": false
        },
        "descriptionModule": {
            "briefSummary": "This study meets the NIH definition of a clinical trial, but is not a treatment study. Instead, the goal of this study is to investigate how hearing ourselves speak affects the planning and execution of speech movements. The study investigates this topic in both typical speakers and in patients with Deep Brain Stimulation (DBS) implants. The main questions it aims to answer are:\n\n* Does the way we hear our own speech while talking affect future speech movements?\n* Can the speech of DBS patients reveal which brain areas are involved in adjusting speech movements? Participants will read words, sentences, or series of random syllables from a computer monitor while their speech is being recorded. For some participants, an electrode cap is also used to record brain activity during these tasks. And for DBS patients, the tasks will be performed with the stimulator ON and with the stimulator OFF."
        },
        "conditionsModule": {
            "conditions": [
                "Speech"
            ]
        },
        "designModule": {
            "studyType": "INTERVENTIONAL",
            "phases": [
                "NA"
            ],
            "designInfo": {
                "allocation": "RANDOMIZED",
                "interventionModel": "FACTORIAL",
                "primaryPurpose": "BASIC_SCIENCE",
                "maskingInfo": {
                    "masking": "NONE"
                }
            },
            "enrollmentInfo": {
                "count": 507,
                "type": "ESTIMATED"
            }
        },
        "armsInterventionsModule": {
            "armGroups": [
                {
                    "label": "Auditory feedback perturbation during speech",
                    "type": "EXPERIMENTAL",
                    "description": "The intervention consists of manipulating real-time auditory feedback during speech production. In our lab, such feedback perturbations can be implemented with either a stand-alone digital vocal processor (a device commonly used by singers and the music industry) or with software-based signal processing routines (see Equipment section for details). Note that the study does not investigate the efficacy of these hardware or software methods to induce behavioral change in subjects' speech. Rather, the study addresses basic experimental questions regarding the general role of auditory feedback in the central nervous system's control of articulatory speech movements.",
                    "interventionNames": [
                        "Behavioral: Auditory feedback perturbation during speech"
                    ]
                },
                {
                    "label": "Visual feedback perturbation during reaching",
                    "type": "EXPERIMENTAL",
                    "description": "The intervention consists of manipulating real-time visual feedback during upper limb reaching movements. In our lab, such feedback perturbations can be implemented with a virtual reality display system.",
                    "interventionNames": [
                        "Behavioral: Visual feedback perturbation during reaching"
                    ]
                },
                {
                    "label": "Deep brain stimulation",
                    "type": "EXPERIMENTAL",
                    "description": "This intervention consists of toggling the deep brain stimulation (DBS) implant ON/OFF prior to participation in the speech auditory-motor learning tasks and speech sequence learning tasks. This intervention can be implemented by the subject themselves as all patients have a hand- held controlled that they use to switch stimulation ON/OFF.",
                    "interventionNames": [
                        "Other: DBS stimulation ON/OFF"
                    ]
                }
            ],
            "interventions": [
                {
                    "type": "BEHAVIORAL",
                    "name": "Auditory feedback perturbation during speech",
                    "description": "The intervention consists of manipulating real-time auditory feedback during speech production. In our lab, such feedback perturbations can be implemented with either a stand-alone digital vocal processor (a device commonly used by singers and the music industry) or with software-based signal processing routines (see Equipment section for details). Note that the study does not investigate the efficacy of these hardware or software methods to induce behavioral change in subjects' speech. Rather, the study addresses basic experimental questions regarding the general role of auditory feedback in the central nervous system's control of articulatory speech movements.",
                    "armGroupLabels": [
                        "Auditory feedback perturbation during speech"
                    ]
                },
                {
                    "type": "BEHAVIORAL",
                    "name": "Visual feedback perturbation during reaching",
                    "description": "The intervention consists of manipulating real-time visual feedback during upper limb reaching movements. In our lab, such feedback perturbations can be implemented with a virtual reality display system.",
                    "armGroupLabels": [
                        "Visual feedback perturbation during reaching"
                    ]
                },
                {
                    "type": "OTHER",
                    "name": "DBS stimulation ON/OFF",
                    "description": "Patients who have been previously implanted with a DBS stimulator for their clinical care will be tested in two speech motor learning tasks with the stimulation ON and with the stimulation OFF.\n\nNote that (1) patients routinely turn the stimulation OFF and back ON (examples are, for some patients, to sleep, to save battery, etc), and (2) we are not in any way evaluating the stimulator itself or its clinical effectiveness but only whether or not two forms of speech motor learning (adaptation to auditory feedback perturbation and speech sequence learning) are affected differently by having the stimulation ON or OFF.\n\nimplant ON/OFF prior to participation in the speech auditory-motor learning tasks and speech sequence learning tasks. This intervention can be implemented by the subject themselves as all patients have a hand- held controlled that they use to switch stimulation ON/OFF.",
                    "armGroupLabels": [
                        "Deep brain stimulation"
                    ]
                }
            ]
        },
        "outcomesModule": {
            "primaryOutcomes": [
                {
                    "measure": "Speech formant frequencies",
                    "description": "The frequencies of the subject's first two formants (F1, F2) for each test word will be measured from spectrographic displays with overlaid Linear Predictive Coding formant tracks.",
                    "timeFrame": "Measurements will be made only from acoustic recordings made during the test session (~1 hour)."
                },
                {
                    "measure": "Reach direction for arm movements",
                    "description": "Measuring initial reach direction for arm movements allows us to measure the direction that was planned before movement onset.",
                    "timeFrame": "Outcome measures will be made only during a single data recording session (~2 hours)."
                },
                {
                    "measure": "Amplitude of long-latency auditory evoked potentials (from EEG recordings) responses",
                    "description": "Amplitude of the N1 component (in microvolt) will be measured in response to both probe tones and to a subject's own speech onset.",
                    "timeFrame": "Measurements will be made only from electroencephalography (EEG) recordings made during the test session (~2 hours)."
                },
                {
                    "measure": "Local field potentials recorded by neural implants",
                    "description": "Local field potentials (LFPs) will be recorded by the PerceptPC DBS implants and used to measure changes in power spectrum density across different phases of the tasks. Additionally, LFPs will be used to conduct event-related analyses.",
                    "timeFrame": "Measurements will be made only from DBS implant recordings made during the test session (~1-2 hours)."
                },
                {
                    "measure": "Temporal measures of speech syllable sequence learning",
                    "description": "1. Speech onset time (in milliseconds); 2. Average syllable duration (in milliseconds)",
                    "timeFrame": "Outcome measures will be made only during a single data recording session (~0.5 hours)"
                },
                {
                    "measure": "Accuracy during speech syllable sequence learning",
                    "description": "Sequence accuracy (in percent)",
                    "timeFrame": "Outcome measures will be made only during a single data recording session (~0.5 hours)"
                }
            ]
        },
        "eligibilityModule": {
            "eligibilityCriteria": "General inclusion criteria:\n\n* native speaker of American English\n* no communication or neurological problems (except for subjects in the DBS group)\n* 250-4000 Hz pure tone hearing thresholds equal to or better than 25 dB HL for children and young adults and equal to or better than 35 dB HL for older adults\n* no medications that affect sensorimotor functioning (except for in the DBS group)\n* adult subjects: 18 years of age or older\n* typical children: 4;0 to 6;11 \\[years;months\\] or 10;0 to 12;11 \\[years;months\\])\n\nSpecific inclusion criteria for children:\n\n\\* scoring above the 20th percentile on the Peabody Picture Vocabulary Test (PPVT-5), Expressive Vocabulary Test (EVT-3), Goldman-Fristoe Test of Articulation (GFTA-3), and either Test of Early Language Development (TELD-4) or (for children age 8 or older) Clinical Evaluation of Language Fundamentals (CELF-5).\n\nSpecific inclusion criteria for DBS patients:\n\n\\* bilateral electrodes implanted in either the ventral intermediate nucleus of the thalamus (Vim; a target site for patients with essential tremor) or subthalamic nucleus (STN; a target site for patients with Parkinson's disease)",
            "healthyVolunteers": true,
            "sex": "ALL",
            "minimumAge": "4 Years",
            "stdAges": [
                "CHILD",
                "ADULT",
                "OLDER_ADULT"
            ]
        },
        "contactsLocationsModule": {
            "centralContacts": [
                {
                    "name": "Ludo Max",
                    "role": "CONTACT",
                    "phone": "206-543-2674",
                    "email": "ludomax@uw.edu"
                }
            ],
            "overallOfficials": [
                {
                    "name": "Ludo Max, Ph.D.",
                    "affiliation": "University of Washington",
                    "role": "PRINCIPAL_INVESTIGATOR"
                }
            ],
            "locations": [
                {
                    "facility": "University of Washington",
                    "status": "RECRUITING",
                    "city": "Seattle",
                    "state": "Washington",
                    "zip": "98105",
                    "country": "United States",
                    "contacts": [
                        {
                            "name": "Patrick Olsen",
                            "role": "CONTACT",
                            "phone": "206-685-7792",
                            "email": "kolsen@uw.edu"
                        }
                    ],
                    "geoPoint": {
                        "lat": 47.60621,
                        "lon": -122.33207
                    }
                }
            ]
        }
    },
    "derivedSection": {
        "miscInfoModule": {
            "versionHolder": "2024-07-30"
        }
    },
    "hasResults": false
}