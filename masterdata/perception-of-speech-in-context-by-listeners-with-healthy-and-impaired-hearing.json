{
    "protocolSection": {
        "identificationModule": {
            "nctId": "NCT06465979",
            "orgStudyIdInfo": {
                "id": "12.0395"
            },
            "secondaryIdInfos": [
                {
                    "id": "R01DC020303",
                    "type": "NIH",
                    "link": "https://reporter.nih.gov/quickSearch/R01DC020303"
                }
            ],
            "organization": {
                "fullName": "University of Louisville",
                "class": "OTHER"
            },
            "briefTitle": "Perception of Speech in Context by Listeners With Healthy and Impaired Hearing",
            "officialTitle": "Perception of Speech in Context by Listeners With Healthy and Impaired Hearing",
            "therapeuticArea": [
                "Other"
            ],
            "study": "perception-of-speech-in-context-by-listeners-with-healthy-and-impaired-hearing"
        },
        "statusModule": {
            "statusVerifiedDate": "2024-06",
            "overallStatus": "RECRUITING",
            "expandedAccessInfo": {
                "hasExpandedAccess": false
            },
            "startDateStruct": {
                "date": "2023-09-18",
                "type": "ACTUAL"
            },
            "primaryCompletionDateStruct": {
                "date": "2026-07-31",
                "type": "ESTIMATED"
            },
            "completionDateStruct": {
                "date": "2027-07-31",
                "type": "ESTIMATED"
            },
            "studyFirstSubmitDate": "2024-06-13",
            "studyFirstSubmitQcDate": "2024-06-18",
            "studyFirstPostDateStruct": {
                "date": "2024-06-20",
                "type": "ACTUAL"
            },
            "lastUpdateSubmitDate": "2024-06-18",
            "lastUpdatePostDateStruct": {
                "date": "2024-06-20",
                "type": "ACTUAL"
            }
        },
        "sponsorCollaboratorsModule": {
            "responsibleParty": {
                "type": "PRINCIPAL_INVESTIGATOR",
                "investigatorFullName": "Christian Stilp",
                "investigatorTitle": "Associate Professor",
                "investigatorAffiliation": "University of Louisville"
            },
            "leadSponsor": {
                "name": "University of Louisville",
                "class": "OTHER"
            },
            "collaborators": [
                {
                    "name": "National Institute on Deafness and Other Communication Disorders (NIDCD)",
                    "class": "NIH"
                }
            ]
        },
        "oversightModule": {
            "oversightHasDmc": false,
            "isFdaRegulatedDrug": false,
            "isFdaRegulatedDevice": false
        },
        "descriptionModule": {
            "briefSummary": "Recognition of speech sounds is accomplished through the use of adjacent sounds in time, in what is termed acoustic context. The frequency and temporal properties of these contextual sounds play a large role in recognition of human speech. Historically, most research on both speech perception and sound perception in general examine sounds out-of-context, or presented individually. Further, these studies have been conducted independently of each other with little connection across labs, across sounds, etc. These approaches slow the progress in understanding how listeners with hearing difficulties use context to recognize speech and how their hearing aids and/or cochlear implants might be modified to improve their perception. This research has three main goals. First, the investigators predict that performance in speech sound recognition experiments will be related when testing the same speech frequencies or the same moments in time, but that performance will not be related in further comparisons across speech frequencies or at different moments in time. Second, the investigators predict that adding background noise will make this contextual speech perception more difficult, and that these difficulties will be more severe for listeners with hearing loss. Third, the investigators predict that cochlear implant users will also use surrounding sounds in their speech recognition, but with key differences than healthy-hearing listeners owing to the sound processing done by their implants. In tandem with these goals, the investigators will use computer models to simulate how neurons respond to speech sounds individually and when surrounded by other sounds.",
            "detailedDescription": "Participants in this study listen to speech played at a comfortable volume and respond by indicating what they heard, either in open-ended form or by choosing among a set of options displayed on a computer. They are seated inside a sound booth and complete the task at their own pace, with little intervention needed from the experimenter. Upon arrival at the lab, participants are given a brief description of the topic of the research (how earlier sounds influence our perception of later speech sounds) and are presented with a detailed informed consent form. Their demographic information is collected and then the experiment is demonstrated. Breaks are offered between testing blocks, which last about 10-15 minutes each.\n\nThe main differences in the protocol consist of the various stimulus manipulations, which are designed to specifically control aspect of the voice that the participant hears. For example, the sound can be manipulated to emphasize higher or lower frequencies, or be spoken relatively slowly or quickly, or manipulated to sound degraded, as if the participant has a hearing loss. In all occasions, the manipulations are signaled to the participant. The outcome measure typically consists of the pattern of word identification, and specifically how that pattern changes depending on acoustic properties of the sounds heard before the target word.\n\nAdditional comparison measurements are taken of the participant's ability to hear and repeat words or sentences in background noise.\n\nOnce an experiment is ready to launch, participants are randomly assigned to different testing conditions (\"interventions\"). But in most of the planned experiments, participants complete identical protocols, except that the specific ordering of many speech stimuli is randomized within the testing session. After the participant completes all of the testing blocks, they are debriefed about the full nature of the study, the hypotheses and the larger scope of the project in the context of speech communication."
        },
        "conditionsModule": {
            "conditions": [
                "Hearing",
                "Hearing Loss"
            ]
        },
        "designModule": {
            "studyType": "INTERVENTIONAL",
            "phases": [
                "NA"
            ],
            "designInfo": {
                "allocation": "NA",
                "interventionModel": "SINGLE_GROUP",
                "primaryPurpose": "BASIC_SCIENCE",
                "maskingInfo": {
                    "masking": "NONE"
                }
            },
            "enrollmentInfo": {
                "count": 680,
                "type": "ESTIMATED"
            }
        },
        "armsInterventionsModule": {
            "armGroups": [
                {
                    "label": "Speech perception experiments",
                    "type": "EXPERIMENTAL",
                    "description": "This arm involves experiments wherein participants listen to speech played at comfortable volumes and respond by indicating what they heard either in open-ended form or by choosing among a set of options displayed on a computer.",
                    "interventionNames": [
                        "Behavioral: Speech Manipulation"
                    ]
                }
            ],
            "interventions": [
                {
                    "type": "BEHAVIORAL",
                    "name": "Speech Manipulation",
                    "description": "The acoustic properties of speech sounds will be modified in two main ways. The first way is to introduce gradual changes to the perceived articulation of the target speech sound, such as changing from \"sh\" to \"s\" by various types of signal processing and filtering. The second type of change is to modify acoustic properties of the sounds that immediately precede the target speech sound, such as changing the speaking rate or its frequencies composition.",
                    "armGroupLabels": [
                        "Speech perception experiments"
                    ]
                }
            ]
        },
        "outcomesModule": {
            "primaryOutcomes": [
                {
                    "measure": "Speech categorization",
                    "description": "The participant sits inside a double-walled sound booth. They are seated at a table that contains a computer monitor and a mouse. A single utterance is played over headphones or through a loudspeaker directly in front, and the participant indicates what they thought the utterance was by selecting among various options on the screen using the mouse. During a single block of trials there are between 60 and 160 sounds, depending on the exact experiment. Ordering of stimuli within the block is randomized. That testing block may be repeated multiple times so that the proportion of responses for each sound-response pair can be estimated reliably and precisely. Completion and advancement of trials is at the participant's own pace. Each testing block takes between 4 and 15 minutes.",
                    "timeFrame": "\"Post-treatment\" where \"treatment\" is the systematic manipulation of speech sounds. Speech categorization will be evaluated during the main part of the testing. Outcomes will be assessed and data reported through study completion, an average of 1 year"
                }
            ],
            "secondaryOutcomes": [
                {
                    "measure": "Speech recognition",
                    "description": "The participant sits inside a double-walled sound booth. They are seated at a table that contains a computer monitor and a mouse. Recorded sentences are played over headphones or through a loudspeaker directly in front, and the participant indicates what they thought the utterance was by typing their response or repeating the sentence aloud. During a single block of trials there are 50 words. Ordering of words within the block is randomized. Completion and advancement of trials is at the participant's own pace. Each testing block takes between 6 and 15 minutes.",
                    "timeFrame": "\"Post-treatment\" where \"treatment\" is the systematic manipulation of speech sounds. Word recognition will be evaluated during the main part of the testing. Outcomes will be assessed and data reported through study completion, an average of 1 year"
                },
                {
                    "measure": "Audiometric threshold testing",
                    "description": "The participant sits inside a double-walled sound booth. Under headphones, they listen for soft tones played by an audiometer. The experimenter controls the timing, frequency and intensity of those tones to find the lowest sound intensity where the participant can detect the tone. Thresholds for sound detection are recorded for octave frequencies between 250 Hz and 8000 Hz.",
                    "timeFrame": "Baseline"
                }
            ]
        },
        "eligibilityModule": {
            "eligibilityCriteria": "Inclusion Criteria:\n\n* Be able to recognize spoken words in English\n* Be a competent speaker of north American English\n* Be an adult between the age of 18 to 65 years\n* Have normal audiometric thresholds below 25 decibels hearing loss (dB HL) at frequencies between 250 and 8000 Hz OR have audiometric thresholds not exceeding 40 dB HL at frequencies between 250 and 8000 Hz OR have audiometric thresholds not exceeding 55 dB HL at frequencies between 250 and 8000 Hz OR use a cochlear implant\n* Lack language-learning or other cognitive disabilities\n\nExclusion Criteria:\n\n* Inability to recognize spoken words in English\n* Not a competent speaker of north American English\n* Be younger than 18 years of age\n* Be older than 65 years of age\n* Have normal audiometric thresholds exceeding 25 dB HL at frequencies between 250 and 8000 Hz OR have audiometric thresholds exceeding 40 dB HL at frequencies between 250 and 8000 Hz OR have audiometric thresholds exceeding 55 dB HL at frequencies between 250 and 8000 Hz\n* Language-learning or other cognitive disabilities",
            "healthyVolunteers": true,
            "sex": "ALL",
            "minimumAge": "18 Years",
            "maximumAge": "65 Years",
            "stdAges": [
                "ADULT",
                "OLDER_ADULT"
            ]
        },
        "contactsLocationsModule": {
            "centralContacts": [
                {
                    "name": "Christian Stilp, PhD",
                    "role": "CONTACT",
                    "phone": "5028520820",
                    "email": "christian.stilp@louisville.edu"
                }
            ],
            "overallOfficials": [
                {
                    "name": "Christian Stilp, PhD",
                    "affiliation": "University of Louisville",
                    "role": "PRINCIPAL_INVESTIGATOR"
                }
            ],
            "locations": [
                {
                    "facility": "University of Louisville",
                    "status": "RECRUITING",
                    "city": "Louisville",
                    "state": "Kentucky",
                    "zip": "40292",
                    "country": "United States",
                    "contacts": [
                        {
                            "name": "Christian Stilp, PhD",
                            "role": "CONTACT",
                            "phone": "502-852-0820",
                            "email": "christian.stilp@louisville.edu"
                        }
                    ],
                    "geoPoint": {
                        "lat": 38.25424,
                        "lon": -85.75941
                    }
                },
                {
                    "facility": "University of Minnesota",
                    "status": "RECRUITING",
                    "city": "Minneapolis",
                    "state": "Minnesota",
                    "zip": "55455",
                    "country": "United States",
                    "contacts": [
                        {
                            "name": "Matthew Winn, PhD, AuD",
                            "role": "CONTACT"
                        }
                    ],
                    "geoPoint": {
                        "lat": 44.97997,
                        "lon": -93.26384
                    }
                }
            ]
        },
        "ipdSharingStatementModule": {
            "ipdSharing": "YES",
            "description": "De-identified data will be made public for purposes of resource sharing and demonstration of research method / analysis. These data, materials used to generate stimuli, and materials used to perform statistical analysis will be made freely available on the Open Science Framework website.",
            "infoTypes": [
                "SAP"
            ],
            "timeFrame": "Data will be made available to all upon acceptance of manuscripts for publication in scholarly journals. Data will be available for a period of at least 10 years or the retirement of both investigators, which is not anticipated to be less than 15 years.",
            "accessCriteria": "Materials will be made public to all upon acceptance of manuscripts for publication in scholarly journals."
        }
    },
    "derivedSection": {
        "miscInfoModule": {
            "versionHolder": "2024-07-30"
        },
        "conditionBrowseModule": {
            "meshes": [
                {
                    "id": "D000034381",
                    "term": "Hearing Loss"
                },
                {
                    "id": "D000003638",
                    "term": "Deafness"
                }
            ],
            "ancestors": [
                {
                    "id": "D000006311",
                    "term": "Hearing Disorders"
                },
                {
                    "id": "D000004427",
                    "term": "Ear Diseases"
                },
                {
                    "id": "D000010038",
                    "term": "Otorhinolaryngologic Diseases"
                },
                {
                    "id": "D000012678",
                    "term": "Sensation Disorders"
                },
                {
                    "id": "D000009461",
                    "term": "Neurologic Manifestations"
                },
                {
                    "id": "D000009422",
                    "term": "Nervous System Diseases"
                }
            ],
            "browseLeaves": [
                {
                    "id": "M24420",
                    "name": "Hearing Loss",
                    "asFound": "Hearing Loss",
                    "relevance": "HIGH"
                },
                {
                    "id": "M6840",
                    "name": "Deafness",
                    "asFound": "Impaired Hearing",
                    "relevance": "HIGH"
                },
                {
                    "id": "M9400",
                    "name": "Hearing Disorders",
                    "relevance": "LOW"
                },
                {
                    "id": "M7601",
                    "name": "Ear Diseases",
                    "relevance": "LOW"
                },
                {
                    "id": "M12961",
                    "name": "Otorhinolaryngologic Diseases",
                    "relevance": "LOW"
                },
                {
                    "id": "M15490",
                    "name": "Sensation Disorders",
                    "relevance": "LOW"
                },
                {
                    "id": "M12404",
                    "name": "Neurologic Manifestations",
                    "relevance": "LOW"
                }
            ],
            "browseBranches": [
                {
                    "abbrev": "BC09",
                    "name": "Ear, Nose, and Throat Diseases"
                },
                {
                    "abbrev": "BC10",
                    "name": "Nervous System Diseases"
                },
                {
                    "abbrev": "BC23",
                    "name": "Symptoms and General Pathology"
                },
                {
                    "abbrev": "All",
                    "name": "All Conditions"
                }
            ]
        }
    },
    "hasResults": false
}